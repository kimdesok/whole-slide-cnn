{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOqUfBcS0iXOJ59Ps5ywqX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdesok/whole-slide-cnn/blob/main/Calc_GPU_memory_requirement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jAk0GWLXoPNR"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Author: Dr. Sreenivas Bhattiprolu \n",
        "Calculation of memory required to store a batch of images and features \n",
        "in a deep learning model. In addition, we also add the memory required\n",
        "to store trainable and non trainable parameters.\n",
        "Remember that you need at least this much memory but in reality you have \n",
        "other overheads. \n",
        "If you do not have this much bare minimum memory then no point in trying to train\n",
        "your model. Consider working with smaller images or batch sizes. \n",
        "\"\"\"\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras import backend as K\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_memory_usage(batch_size, model):\n",
        "    \n",
        "    features_mem = 0 # Initialize memory for features. \n",
        "    float_bytes = 4.0 #Multiplication factor as all values we store would be float32.\n",
        "    \n",
        "    for layer in model.layers:\n",
        "\n",
        "        out_shape = layer.output_shape\n",
        "        \n",
        "        if type(out_shape) is list:   #e.g. input layer which is a list\n",
        "            out_shape = out_shape[0]\n",
        "        else:\n",
        "            out_shape = [out_shape[1], out_shape[2], out_shape[3]]\n",
        "            \n",
        "        #Multiply all shapes to get the total number per layer.    \n",
        "        single_layer_mem = 1 \n",
        "        for s in out_shape:\n",
        "            if s is None:\n",
        "                continue\n",
        "            single_layer_mem *= s\n",
        "        \n",
        "        single_layer_mem_float = single_layer_mem * float_bytes #Multiply by 4 bytes (float)\n",
        "        single_layer_mem_MB = single_layer_mem_float/(1024**2)  #Convert to MB\n",
        "        \n",
        "        #print(\"Memory for\", out_shape, \" layer in MB is:\", single_layer_mem_MB)\n",
        "        features_mem += single_layer_mem_MB  #Add to total feature memory count\n",
        "\n",
        "# Calculate Parameter memory\n",
        "    trainable_wts = np.sum([K.count_params(p) for p in model.trainable_weights])\n",
        "    non_trainable_wts = np.sum([K.count_params(p) for p in model.non_trainable_weights])\n",
        "    parameter_mem_MB = ((trainable_wts + non_trainable_wts) * float_bytes)/(1024**2)\n",
        "    print(\"----------------------------------------\")\n",
        "    print(\"Memory for features in GB is: %.2f\" %(features_mem*batch_size/1024))\n",
        "    print(\"Memory for parameters in GB is: %.2f\" %(parameter_mem_MB/1024))\n",
        "\n",
        "    total_memory_MB = (batch_size * features_mem) + parameter_mem_MB  #Same number of parameters. independent of batch size\n",
        "\n",
        "    total_memory_GB = total_memory_MB/1024\n",
        "    \n",
        "    return total_memory_GB"
      ],
      "metadata": {
        "id": "TtdbNxkQoSRw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let us import VGG16 model.\n",
        "input_image_shape = (10000, 10000, 3)\n",
        "#model = VGG16(include_top=False, input_shape=input_image_shape)\n",
        "model = ResNet50(include_top=False, input_shape=input_image_shape)\n",
        "#print(model.summary())\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "##############################################################\n",
        "batch_size = BATCH_SIZE\n",
        "mem_for_my_model = get_model_memory_usage(batch_size, model)\n",
        "\n",
        "print(\"----------------------------------------\")\n",
        "print(\"Minimum memory required to work with this model is: %.2f\" %mem_for_my_model, \"GB\")\n",
        "\n",
        "\n",
        "###############################################################"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRGr8ftToc2R",
        "outputId": "2ccb4ea5-7b0f-49da-c0c0-360c5a6a0dce"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "Memory for features in GB is: 275.35\n",
            "Memory for parameters in GB is: 0.09\n",
            "----------------------------------------\n",
            "Minimum memory required to work with this model is: 275.44 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Expense estimation in case purchasing more RAM\n",
        "my_mem = 410; time_to_run = 150; won_per_hour_per_GB = 1.6\n",
        "expense = (mem_for_my_model - my_mem);print(\"Lacking this much memory, %d GB \" % int(expense))\n",
        "expense = expense * time_to_run * won_per_hour_per_GB\n",
        "print(\"Needs this much money, %d won per job\" % int(expense))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yLbgTLCSOML",
        "outputId": "e159111c-f729-404f-a5ba-be825ec88961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lacking this much memory, 922 GB \n",
            "Needs this much money, 221434 won per job\n"
          ]
        }
      ]
    }
  ]
}